---
schema-version: v1.2.3
id: draft-liu-nmrg-ai-llm-inference-requirements-01
title:
- content: Requirements Analysis of System and Network for Large Language Model Inference
    Service
  language:
  - en
  script:
  - Latn
  format: text/plain
link:
- content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-01
  type: src
type: standard
docid:
- id: draft-liu-nmrg-ai-llm-inference-requirements-01
  type: Internet-Draft
  primary: true
- id: I-D.liu-nmrg-ai-llm-inference-requirements
  type: IETF
  scope: anchor
docnumber: I-D.liu-nmrg-ai-llm-inference-requirements
date:
- type: published
  value: '2025-07-07'
contributor:
- person:
    name:
      given:
        forename:
        - content: Liu
          language:
          - en
        - language:
          - en
          initial: L
        formatted_initials:
          content: L.
          language:
          - en
      surname:
        content: Chang
        language:
        - en
      completename:
        content: Liu Chang
        language:
        - en
    affiliation:
    - organization:
        name:
        - content: China Mobile
  role:
  - type: author
- person:
    name:
      given:
        forename:
        - content: Chuyi
          language:
          - en
        - language:
          - en
          initial: C
        formatted_initials:
          content: C.
          language:
          - en
      surname:
        content: Guo
        language:
        - en
      completename:
        content: Chuyi Guo
        language:
        - en
    affiliation:
    - organization:
        name:
        - content: China Mobile
  role:
  - type: author
version:
- draft: '01'
revdate: '2025-07-07'
language:
- en
script:
- Latn
abstract:
- content: "<p>With the rise of ChatGPT, DeepSeek, and other Large Language Models,
    which is short for LLMs in the remaining part, as well as the proliferation of
    inference applications, inference serving oriented to large-scale users has become
    increasingly critical. However, due to the extreme demands on computing power
    and communication during inference, the large-scale service deployment of LLMs
    poses significant challenges. To address these challenges, different vendors have
    adopted diverse inference service architectures, such as vLLM, SGLang, Mooncake,
    etc. This paper investigates mainstream inference frameworks, summarizes their
    core design principle and research question, and analyzes the challenges and requirements
    they impose on network management. The goal is to lay a foundation for defining
    a unified LLM inference architecture in the future.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: updates
  bibitem:
    id: draft-liu-nmrg-ai-llm-inference-requirements-00
    docid:
    - id: draft-liu-nmrg-ai-llm-inference-requirements-00
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-liu-nmrg-ai-llm-inference-requirements-00
      format: text/plain
series:
- type: main
  title:
    content: Internet-Draft
    language:
    - en
    script:
    - Latn
    format: text/plain
  number: draft-liu-nmrg-ai-llm-inference-requirements-01
doctype: internet-draft
ext:
  schema-version: v1.0.1
