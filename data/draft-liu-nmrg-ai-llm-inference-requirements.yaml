---
schema-version: v1.2.3
id: draft-liu-nmrg-ai-llm-inference-requirements
title:
- content: Requirements Analysis of System and Network for Large Language Model Inference
    Service
  language:
  - en
  script:
  - Latn
  format: text/plain
docid:
- id: draft-liu-nmrg-ai-llm-inference-requirements
  type: Internet-Draft
  primary: true
abstract:
- content: "<p>With the rise of ChatGPT, DeepSeek, and other Large Language Models,
    which is short for LLMs in the remaining part, as well as the proliferation of
    inference applications, inference serving oriented to large-scale users has become
    increasingly critical. However, due to the extreme demands on computing power
    and communication during inference, the large-scale service deployment of LLMs
    poses significant challenges. To address these challenges, different vendors have
    adopted diverse inference service architectures, among which the vLLM proposed
    in 2023 is the most representative. This paper investigates mainstream inference
    frameworks, summarizes their core design principles, and analyzes the requirements
    and challenges they impose on system and network configurations. The goal is to
    lay a foundation for defining a unified LLM inference architecture in the future.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: includes
  bibitem:
    id: draft-liu-nmrg-ai-llm-inference-requirements-00
    docid:
    - id: draft-liu-nmrg-ai-llm-inference-requirements-00
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-liu-nmrg-ai-llm-inference-requirements-00
      format: text/plain
ext:
  schema-version: v1.0.1
