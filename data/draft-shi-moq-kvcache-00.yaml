---
schema-version: v1.2.3
id: draft-shi-moq-kvcache-00
title:
- content: KVCache over MoQT
  language:
  - en
  script:
  - Latn
  format: text/plain
link:
- content: https://datatracker.ietf.org/doc/html/draft-shi-moq-kvcache-00
  type: src
type: standard
docid:
- id: draft-shi-moq-kvcache-00
  type: Internet-Draft
  primary: true
- id: I-D.shi-moq-kvcache
  type: IETF
  scope: anchor
docnumber: I-D.shi-moq-kvcache
date:
- type: published
  value: '2025-03-03'
contributor:
- person:
    name:
      given:
        forename:
        - content: Hang
          language:
          - en
        - language:
          - en
          initial: H
        formatted_initials:
          content: H.
          language:
          - en
      surname:
        content: Shi
        language:
        - en
      completename:
        content: Hang Shi
        language:
        - en
    affiliation:
    - organization:
        name:
        - content: Huawei Technologies
  role:
  - type: author
version:
- draft: '00'
revdate: '2025-03-03'
language:
- en
script:
- Latn
abstract:
- content: "<p>Large language model (LLM) inference involves two stages: prefill and
    decode. The prefill phase processes the prompt in parallel, generating the KVCache,
    which is then used by the decode phase to produce tokens sequentially. KVCache
    can be reused if the model and prompt is the same, reducing computing cost of
    the prefill. However, its large size makes efficient transfer challenging. Delivering
    these over architectures enabled by publish/subscribe transport like MoQT, allows
    local nodes to cache the KVCache to be later retrieved via new subscriptions,
    saving the bandwidth. This document specifies the transmission of KVCache over
    MoQT.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
series:
- type: main
  title:
    content: Internet-Draft
    language:
    - en
    script:
    - Latn
    format: text/plain
  number: draft-shi-moq-kvcache-00
doctype: internet-draft
ext:
  schema-version: v1.0.1
