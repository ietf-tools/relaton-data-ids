---
schema-version: v1.2.3
id: draft-liu-nmrg-ai-llm-inference-requirements-00
title:
- content: Requirements Analysis of System and Network for Large Language Model Inference
    Service
  language:
  - en
  script:
  - Latn
  format: text/plain
link:
- content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-00
  type: src
type: standard
docid:
- id: draft-liu-nmrg-ai-llm-inference-requirements-00
  type: Internet-Draft
  primary: true
- id: I-D.liu-nmrg-ai-llm-inference-requirements
  type: IETF
  scope: anchor
docnumber: I-D.liu-nmrg-ai-llm-inference-requirements
date:
- type: published
  value: '2025-03-03'
contributor:
- person:
    name:
      given:
        forename:
        - content: Liu
          language:
          - en
        - language:
          - en
          initial: L
        formatted_initials:
          content: L.
          language:
          - en
      surname:
        content: Chang
        language:
        - en
      completename:
        content: Liu Chang
        language:
        - en
    affiliation:
    - organization:
        name:
        - content: China Mobile
  role:
  - type: author
- person:
    name:
      given:
        forename:
        - content: Chuyi
          language:
          - en
        - language:
          - en
          initial: C
        formatted_initials:
          content: C.
          language:
          - en
      surname:
        content: Guo
        language:
        - en
      completename:
        content: Chuyi Guo
        language:
        - en
    affiliation:
    - organization:
        name:
        - content: China Mobile
  role:
  - type: author
version:
- draft: '00'
revdate: '2025-03-03'
language:
- en
script:
- Latn
abstract:
- content: "<p>With the rise of ChatGPT, DeepSeek, and other Large Language Models,
    which is short for LLMs in the remaining part, as well as the proliferation of
    inference applications, inference serving oriented to large-scale users has become
    increasingly critical. However, due to the extreme demands on computing power
    and communication during inference, the large-scale service deployment of LLMs
    poses significant challenges. To address these challenges, different vendors have
    adopted diverse inference service architectures, among which the vLLM proposed
    in 2023 is the most representative. This paper investigates mainstream inference
    frameworks, summarizes their core design principles, and analyzes the requirements
    and challenges they impose on system and network configurations. The goal is to
    lay a foundation for defining a unified LLM inference architecture in the future.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: updatedBy
  bibitem:
    id: draft-liu-nmrg-ai-llm-inference-requirements-01
    docid:
    - id: draft-liu-nmrg-ai-llm-inference-requirements-01
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-liu-nmrg-ai-llm-inference-requirements-01
      format: text/plain
series:
- type: main
  title:
    content: Internet-Draft
    language:
    - en
    script:
    - Latn
    format: text/plain
  number: draft-liu-nmrg-ai-llm-inference-requirements-00
doctype: internet-draft
ext:
  schema-version: v1.0.1
