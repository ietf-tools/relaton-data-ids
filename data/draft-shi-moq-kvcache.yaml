---
schema-version: v1.2.3
id: draft-shi-moq-kvcache
title:
- content: KVCache over MoQT
  language:
  - en
  script:
  - Latn
  format: text/plain
docid:
- id: draft-shi-moq-kvcache
  type: Internet-Draft
  primary: true
abstract:
- content: "<p>Large language model (LLM) inference involves two stages: prefill and
    decode. The prefill phase processes the prompt in parallel, generating the KVCache,
    which is then used by the decode phase to produce tokens sequentially. KVCache
    can be reused if the model and prompt is the same, reducing computing cost of
    the prefill. However, its large size makes efficient transfer challenging. Delivering
    these over architectures enabled by publish/subscribe transport like MoQT, allows
    local nodes to cache the KVCache to be later retrieved via new subscriptions,
    saving the bandwidth. This document specifies the transmission of KVCache over
    MoQT.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: includes
  bibitem:
    id: draft-shi-moq-kvcache-00
    docid:
    - id: draft-shi-moq-kvcache-00
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-shi-moq-kvcache-00
      format: text/plain
- type: includes
  bibitem:
    id: draft-shi-moq-kvcache-01
    docid:
    - id: draft-shi-moq-kvcache-01
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-shi-moq-kvcache-01
      format: text/plain
ext:
  schema-version: v1.0.1
