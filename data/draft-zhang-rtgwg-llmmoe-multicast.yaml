---
schema-version: v1.2.3
id: draft-zhang-rtgwg-llmmoe-multicast
title:
- content: Multicast usage in LLM MoE
  language:
  - en
  script:
  - Latn
  format: text/plain
docid:
- id: draft-zhang-rtgwg-llmmoe-multicast
  type: Internet-Draft
  primary: true
abstract:
- content: "<p>Large Language Models (LLMs) have been widely used in recent years.
    The Mixture of Experts (MoE) architecture is one of the features of LLMs that
    enables efficient inference and cost-effective training. With the MoE architecture,
    there are potential multicast use cases such as tokens dispatching. This draft
    attempts to analyze these use cases.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: includes
  bibitem:
    id: draft-zhang-rtgwg-llmmoe-multicast-00
    docid:
    - id: draft-zhang-rtgwg-llmmoe-multicast-00
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-zhang-rtgwg-llmmoe-multicast-00
      format: text/plain
ext:
  schema-version: v1.0.1
